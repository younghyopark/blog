<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using Demonstrations with RL | Younghyo's Blog</title><meta name=keywords content><meta name=description content="Introduction Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.
Learning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way &ndash; it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab &ndash; outside the data distribution &ndash; the trained policy is highly likely to fail."><meta name=author content="Younghyo Park"><link rel=canonical href=https://younghyopark.me/blog/posts/demonstration-and-rl/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.3f686b32a01896608101ddd02e74a3e844730b49647eed116121d0884272d7f2.css integrity="sha256-P2hrMqAYlmCBAd3QLnSj6ERzC0lkfu0RYSHQiEJy1/I=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Using Demonstrations with RL"><meta property="og:description" content="Introduction Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.
Learning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way &ndash; it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab &ndash; outside the data distribution &ndash; the trained policy is highly likely to fail."><meta property="og:type" content="article"><meta property="og:url" content="https://younghyopark.me/blog/posts/demonstration-and-rl/"><meta property="og:image" content="https://younghyopark.me/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-07T15:45:14-05:00"><meta property="article:modified_time" content="2024-01-07T15:45:14-05:00"><meta property="og:site_name" content="Younghyo's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://younghyopark.me/blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Using Demonstrations with RL"><meta name=twitter:description content="Introduction Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.
Learning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way &ndash; it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab &ndash; outside the data distribution &ndash; the trained policy is highly likely to fail."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://younghyopark.me/blog/posts/"},{"@type":"ListItem","position":3,"name":"Using Demonstrations with RL","item":"https://younghyopark.me/blog/posts/demonstration-and-rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using Demonstrations with RL","name":"Using Demonstrations with RL","description":"Introduction Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.\nLearning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way \u0026ndash; it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab \u0026ndash; outside the data distribution \u0026ndash; the trained policy is highly likely to fail.","keywords":[],"articleBody":"Introduction Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.\nLearning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way ‚Äì it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab ‚Äì outside the data distribution ‚Äì the trained policy is highly likely to fail.\nReinforcement learning (RL), on the other hand, is way better at creating robust policies by actively exploring the world for better actions. However, although it might be not be physically taxing, engineering an RL policy can still be a mentally taxing process. It requires carefully creating the simulation environment, shaping the reward/action/state space to make the agent efficiently explore, and most importantly, it is not an intuitive process to do which often requires multiple engineering iterations.\nCombining demonstration dataset for reinforcement learning ‚Äì LfD x RL ‚Äì is thus very promising in many ways. Demonstration can serve as a good guidance for the agent while exploring the world, and such exploration can help us achieve the level of robustness that cannot be achieved training solely on demonstration.\nPossible workflows Although it‚Äôs often overlooked in many LfD x RL papers, there are many ways how demonstrations can be combined with RL in the context of real-world robotics deployment. Based on how we collect the demonstration and where the agent explores, six type of workflows can be studied.\nDemonstrate in ‚Üí Explore in ‚Üì Simulation\n(w/ robot action) Real-world\n(w/ human action) Real-world\n(w/ robot action) Simulation type A ‚òÖ type B type C Real-world type D type E type F ‚òÖ Table 1. Possible workflows for demonstration + RL Demonstrate and Explore in the same world Type A and Type F is the case where most of the existing LfD x RL literatures deal with ‚Äî a workflow where the demonstration and exploration happens in the same world. It is the most clean setup to study since both are subject to the same governing physics, same state/action space, and the same environment setup. However, both of these approach has some practical limitations.\nType A : Data collection and RL exploration is all done in simulation.\nData Collection in Sim Exploration in Sim Pros üëç data collection pipeline can leverage simulation privileges (e.g. easy resets, automated data collection or post-augmentation using oracles) exploration cost is cheap and can leverage many simulation privileges (e.g. resets, oracle states) Cons üëé cannot collect demonstrations for tasks that cannot be simulated (e.g. deformables) final policy lives in a simulation ‚Äî Sim2Real is required! Type F : Data collection and RL exploration is all done in real-world.\nData Collection in Real-world Exploration in Real-world Pros üëç can collect demonstrations for any tasks that law of physics allowcan collect high-quality demonstrations with real-world data points no need to deal with sim-to-real issues: the final policy lives in the real-world! Cons üëé cost of data collection can be expensive ‚Äì cannot be sped up, the environment should be manually be resetted every time all the hassles involved with doing real-world RL (e.g. manual resets‚Ä¶) comes back. Demonstrate and Explore in different worlds One other promising alternative is type C, which can be a practical choice to consider when collecting real-world demonstration is relatively easier.\nType C : Data is collected in real-world, but exploration is done in simulation. This naturally combines the strength of each approach ‚Äì leveraging realistic, high-quality demonstrations in real-world, and cheap interaction costs of the simulation.\nData Collection in Real-world Exploration in Simulation Pros üëç - can collect demonstrations for any tasks that law of physics allows - can collect high-quality demonstrations with real-world data points no need to deal with sim-to-real issues: the final policy lives in the real-world! Cons üëé cost of data collection can be expensive ‚Äì cannot be sped up, the environment should be manually be resetted every time all the hassles involved with doing real-world RL (e.g. manual resets‚Ä¶) comes back. data collection: üëé : demonstration tasks are being limited by simulation capabilities (since the exploration happens in the simulation) üëé : cost of data collection can be expensive ‚Äì cannot be sped up, the environment should be manually be resetted every time üëé : should deal with discrepancies between real-world vs simulation environment setup ‚Äì either being the discrepancy in physics, asset modeling, ‚Ä¶ type F also does fall into this category, but it is a bit more interesting:\nType F: Data is collected in simulation, but exploration is done in real-world. Demonstrate and Explore with different morphology Methods Offline Usage Online Usage Practical Challenges Conclusion ","wordCount":"808","inLanguage":"en","datePublished":"2024-01-07T15:45:14-05:00","dateModified":"2024-01-07T15:45:14-05:00","author":{"@type":"Person","name":"Younghyo Park"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://younghyopark.me/blog/posts/demonstration-and-rl/"},"publisher":{"@type":"Organization","name":"Younghyo's Blog","logo":{"@type":"ImageObject","url":"https://younghyopark.me/blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://younghyopark.me/blog accesskey=h title="Younghyo's Blog (Alt + H)"><img src=https://younghyopark.me/apple-touch-icon.png alt aria-label=logo height=35>Younghyo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://younghyopark.me/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://younghyopark.me/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://younghyopark.me/ title="About Me"><span>About Me</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://younghyopark.me/blog>Home</a>&nbsp;¬ª&nbsp;<a href=https://younghyopark.me/blog/posts/>Posts</a></div><h1 class=post-title>Using Demonstrations with RL</h1><div class=post-meta><span title='2024-01-07 15:45:14 -0500 EST'>January 7, 2024</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;808 words&nbsp;¬∑&nbsp;Younghyo Park</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp; &nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#possible-workflows>Possible workflows</a><ul><li><a href=#demonstrate-and-explore-span-classyellowin-the-same-worldspan>Demonstrate and Explore <span class=yellow>in the same world</span></a></li><li><a href=#demonstrate-and-explore-span-classyellowin-different-worldsspan>Demonstrate and Explore <span class=yellow>in different worlds</span></a></li><li><a href=#demonstrate-and-explore-span-classyellowwith-different-morphologyspan>Demonstrate and Explore <span class=yellow>with different morphology</span></a></li></ul></li><li><a href=#methods>Methods</a><ul><li><a href=#offline-usage>Offline Usage</a></li><li><a href=#online-usage>Online Usage</a></li></ul></li><li><a href=#practical-challenges>Practical Challenges</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Learning from demonstation (LfD) and Reinforcement learning (RL) has its own strengths and caveats.</p><p>Learning a policy from demonstration (LfD) is the simplest yet effective way of doing robot learning. A simple behavior cloning, with a few tricks, can definitely go a long way &ndash; it can generate a policy for relatively complex manipulation tasks with just a few demonstrations. However, relying soley on demonstration alone can be problematic when it comes to the issue of domain shift: when the robot goes outside of the lab &ndash; outside the data distribution &ndash; the trained policy is highly likely to fail.</p><p>Reinforcement learning (RL), on the other hand, is way better at creating robust policies by actively exploring the world for better actions. However, although it might be not be <em>physically</em> taxing, engineering an RL policy can still be a <em>mentally</em> taxing process. It requires carefully creating the simulation environment, shaping the reward/action/state space to make the agent efficiently explore, and most importantly, it is not an intuitive process to do which often requires multiple engineering iterations.</p><p>Combining demonstration dataset for reinforcement learning &ndash; LfD x RL &ndash; is thus very promising in many ways. <span style=color:green>Demonstration can serve as a good guidance for the agent while exploring the world,</span> and such exploration can help us <span style=color:green>achieve the level of robustness that cannot be achieved training solely on demonstration.</span></p><h2 id=possible-workflows>Possible workflows<a hidden class=anchor aria-hidden=true href=#possible-workflows>#</a></h2><p>Although it&rsquo;s often overlooked in many LfD x RL papers, there are many ways how demonstrations can be combined with RL in the context of real-world robotics deployment. Based on <em>how</em> we collect the demonstration and <em>where</em> the agent explores, six type of workflows can be studied.</p><style type=text/css>.tg{border-collapse:collapse;border-spacing:0;margin:0 auto;border-bottom:1px solid;width:700px;vertical-align:center}.tg td{border-color:#000;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;vertical-align:center;overflow:hidden;padding:10px 5px;word-break:normal;width:750px}.tg th{border-color:#000;border-style:solid;border-width:1px;font-family:Arial,sans-serif;font-size:14px;font-weight:400;overflow:hidden;padding:10px 5px;word-break:normal;width:750px;vertical-align:center}.tg .tg-b2ru{background-color:#cbcefb;border-color:#000;color:#343434;font-size:100%;text-align:center;vertical-align:center}.tg .tg-mc6y{background-color:#fff;border-color:#000;font-size:100%;text-align:center;vertical-align:center}.tg .tg-s1i5{background-color:#ffccc9;border-color:#000;color:#343434;font-size:100%;text-align:center;vertical-align:center}.tg .tg-jg3o{background-color:#fff;border-color:#000;color:#343434;font-size:100%;font-weight:700;text-align:center;vertical-align:top}@media screen and (max-width:767px){.tg{width:auto!important}.tg col{width:auto!important}.tg-wrap{overflow-x:auto;-webkit-overflow-scrolling:touch;margin:auto 0}}</style><div class=tg-wrap><table class=tg><thead><tr><th class=tg-mc6y><span style=font-weight:700;color:#3166ff>Demonstrate in ‚Üí<br></span><span style=font-weight:700;color:#cb0000>Explore in ‚Üì</span></th><th class=tg-b2ru><span style=font-weight:700;font-style:normal>Simulation<br>(w/ robot action)</span></th><th class=tg-b2ru><span style=font-weight:700;font-style:normal>Real-world<br>(w/ human action)</span></th><th class=tg-b2ru><span style=font-weight:700;font-style:normal>Real-world<br>(w/ robot action)</span></th></tr></thead><tbody><tr><td class=tg-s1i5><span style=font-weight:700>Simulation</span></td><td class=tg-jg3o>type A ‚òÖ</td><td class=tg-jg3o>type B</td><td class=tg-jg3o>type C</td></tr><tr><td class=tg-s1i5><span style=font-weight:700>Real-world</span></td><td class=tg-jg3o>type D</td><td class=tg-jg3o>type E</td><td class=tg-jg3o>type F ‚òÖ</td></tr></tbody><caption>Table 1. Possible workflows for demonstration + RL</caption></table></div><h3 id=demonstrate-and-explore-span-classyellowin-the-same-worldspan>Demonstrate and Explore <span class=yellow>in the same world</span><a hidden class=anchor aria-hidden=true href=#demonstrate-and-explore-span-classyellowin-the-same-worldspan>#</a></h3><p><strong>Type A</strong> and <strong>Type F</strong> is the case where most of the existing LfD x RL literatures deal with &mdash; a workflow where the demonstration and exploration happens in the same world. It is the most clean setup to study since both are subject to the same governing physics, same state/action space, and the same environment setup. However, both of these approach has some practical limitations.</p><ul><li><p><strong>Type A</strong> : <u>Data collection and RL exploration is all done in <strong>simulation</strong>.</u></p><table><thead><tr><th style=text-align:center></th><th>Data Collection in Sim</th><th>Exploration in Sim</th></tr></thead><tbody><tr><td style=text-align:center>Pros üëç</td><td><ul><li>data collection pipeline can leverage simulation privileges (e.g. easy resets, automated data collection or post-augmentation using oracles)</li></ul></td><td><ul><li>exploration cost is cheap and can leverage many simulation privileges<br>(e.g. resets, oracle states)</li></ul></td></tr><tr><td style=text-align:center>Cons üëé</td><td><ul><li>cannot collect demonstrations for tasks that cannot be simulated (e.g. deformables)</li></ul></td><td><ul><li>final policy lives in a simulation &mdash; Sim2Real is required!</li></ul></td></tr></tbody></table></li><li><p><strong>Type F</strong> : <u>Data collection and RL exploration is all done in <strong>real-world</strong>.</u></p><table><thead><tr><th style=text-align:center></th><th><center>Data Collection in Real-world</center></th><th><center>Exploration in Real-world</center></th></tr></thead><tbody><tr><td style=text-align:center>Pros üëç</td><td><ul><li>can collect demonstrations for any tasks that law of physics allow</li><li>can collect high-quality demonstrations with real-world data points</li></ul></td><td><ul><li>no need to deal with sim-to-real issues: the final policy lives in the real-world!</li></ul></td></tr><tr><td style=text-align:center>Cons üëé</td><td><ul><li>cost of data collection can be expensive &ndash; cannot be sped up, the environment should be manually be resetted every time</li></ul></td><td><ul><li>all the hassles involved with doing <strong>real-world RL</strong> (e.g. manual resets&mldr;) comes back.</li></ul></td></tr></tbody></table></li></ul><h3 id=demonstrate-and-explore-span-classyellowin-different-worldsspan>Demonstrate and Explore <span class=yellow>in different worlds</span><a hidden class=anchor aria-hidden=true href=#demonstrate-and-explore-span-classyellowin-different-worldsspan>#</a></h3><p>One other promising alternative is <strong>type C</strong>, which can be a practical choice to consider when collecting real-world demonstration is relatively easier.</p><ul><li><p><strong>Type C</strong> : <u>Data is collected in <strong>real-world</strong>, but exploration is done in <strong>simulation</strong>.</u> This naturally combines the strength of each approach &ndash; leveraging realistic, high-quality demonstrations in real-world, and cheap interaction costs of the simulation.</p><table><thead><tr><th style=text-align:center></th><th>Data Collection in Real-world</th><th>Exploration in Simulation</th></tr></thead><tbody><tr><td style=text-align:center>Pros üëç</td><td>- can collect demonstrations for any tasks that law of physics allows<br>- can collect high-quality demonstrations with real-world data points</td><td>no need to deal with sim-to-real issues: the final policy lives in the real-world!</td></tr><tr><td style=text-align:center>Cons üëé</td><td>cost of data collection can be expensive &ndash; cannot be sped up, the environment should be manually be resetted every time</td><td>all the hassles involved with doing <strong>real-world RL</strong> (e.g. manual resets&mldr;) comes back.</td></tr></tbody></table><ul><li>data collection:<ul><li>üëé : demonstration tasks are being limited by simulation capabilities (since the exploration happens in the simulation)</li><li>üëé : cost of data collection can be expensive &ndash; cannot be sped up, the environment should be manually be resetted every time</li></ul></li><li></li><li>üëé : should deal with discrepancies between real-world vs simulation environment setup &ndash; either being the discrepancy in physics, asset modeling, &mldr;</li></ul></li></ul><p><strong>type F</strong> also does fall into this category, but it is a bit more interesting:</p><ul><li><strong>Type F</strong>: <u>Data is collected in <strong>simulation</strong>, but exploration is done in <strong>real-world</strong>.</u></li></ul><h3 id=demonstrate-and-explore-span-classyellowwith-different-morphologyspan>Demonstrate and Explore <span class=yellow>with different morphology</span><a hidden class=anchor aria-hidden=true href=#demonstrate-and-explore-span-classyellowwith-different-morphologyspan>#</a></h3><h2 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h2><h3 id=offline-usage>Offline Usage<a hidden class=anchor aria-hidden=true href=#offline-usage>#</a></h3><h3 id=online-usage>Online Usage<a hidden class=anchor aria-hidden=true href=#online-usage>#</a></h3><h2 id=practical-challenges>Practical Challenges<a hidden class=anchor aria-hidden=true href=#practical-challenges>#</a></h2><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://younghyopark.me/blog/posts/automating-behavior-generation/><span class=title>Next ¬ª</span><br><span>Automating Solvers for Behavior Generation</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on twitter" href="https://twitter.com/intent/tweet/?text=Using%20Demonstrations%20with%20RL&amp;url=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f&amp;title=Using%20Demonstrations%20with%20RL&amp;summary=Using%20Demonstrations%20with%20RL&amp;source=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f&title=Using%20Demonstrations%20with%20RL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on whatsapp" href="https://api.whatsapp.com/send?text=Using%20Demonstrations%20with%20RL%20-%20https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Using Demonstrations with RL on telegram" href="https://telegram.me/share/url?text=Using%20Demonstrations%20with%20RL&amp;url=https%3a%2f%2fyounghyopark.me%2fblog%2fposts%2fdemonstration-and-rl%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://younghyopark.me/blog>Younghyo's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>